# VQA-with-Audio using BLIP and Gradio
## Project Description
This project aims to develop an interactive application that answers visual questions based on an image uploaded by the user. The application utilizes a pre-trained model from Hugging Face's BLIP (Bootstrapped Language Image Pre-training) series, allowing users to ask questions related to the uploaded image. The response is generated audio formats, providing an engaging and versatile user experience.

## Features:
- Image Upload: Users can upload an image, which will be analyzed by the AI model.
- Question Input: Users can ask a question related to the content of the uploaded image.
- Audio Response: The application generates an audio file of the answer, making the interaction more dynamic and accessible.
- Simple User Interface: Gradio is used to create an easy-to-use interface where users can upload images, ask questions, and receive answers in an audio format.
## Tools and Technologies:
- Hugging Face Transformers: Utilizing the BLIP model for Visual Question Answering (VQA) capabilities.
- Gradio: Creating an intuitive interface for image upload and question input with audio output.
- Google Text-to-Speech (gTTS): Converting text answers into spoken audio for a richer user experience.
- Python: The primary language for integrating the model, interface, and audio conversion.
- GitHub: For code versioning and project collaboration.
## The Colab URLs:
- Full-Implementation: [Colab Notebook Link](https://colab.research.google.com/drive/1nBDQaz2k_4L3zv8b8FUp309pdEFrA0jV?usp=sharing)
- Gradio Example: [Gradio Implementation](https://huggingface.co/spaces/saja003/VQA-with-Audio)
## The Video URL:
- Project Demo Video
## Objectives:
- Enable users to interact with images through natural language questions.
- Provide audio responses to make the application more engaging and accessible.
- Develop a user-friendly interface that simplifies interaction with advanced AI models.
